# E-commerce Data Pipeline (PySpark)  
**Name:** Etsubdink Tadesse  
**ID:** DBUR/3971/13  

ğŸ“Œ **Overview**  
This project implements an **end-to-end data pipeline** using **PySpark** and **DuckDB** for processing e-commerce transactions. The pipeline efficiently extracts, transforms, and loads (ETL) data.

---

## ğŸš€ Project Workflow

1ï¸âƒ£ **Data Extraction**  
   - Loads raw e-commerce data from a **CSV file** using **PySpark**.

2ï¸âƒ£ **Data Transformation**  
   - Cleans and preprocesses data, handling missing values and filtering invalid records.  
   - Categorizes transactions based on price into **High, Medium, and Low Value**.

3ï¸âƒ£ **Data Loading**  
   - Stores transformed data into **DuckDB**, a lightweight OLAP database.

---

## ğŸ“‚ Directory Structure
â”œâ”€â”€ data/ # Raw and processed datasets â”œâ”€â”€ scripts/ # Python scripts for ETL pipeline â”‚ â”œâ”€â”€ extract.py # Extracts data from CSV â”‚ â”œâ”€â”€ transform.py # Cleans and transforms data â”‚ â”œâ”€â”€ load.py # Loads data into DuckDB â”‚ â”œâ”€â”€ notebooks/ # Jupyter notebooks for analysis â”œâ”€â”€ visualizations/ # Tableau dashboards & insights â”œâ”€â”€ README.md # Project documentation

---

## âš™ï¸ How to Run

### ğŸ”¹ **Setup Environment**
Ensure Python 3.x is installed, then install dependencies:

```sh
pip install pyspark duckdb
ğŸ”¹ Run the Pipeline
Execute the ETL pipeline step-by-step:
python3 scripts/extract.py
python3 scripts/transform.py
python3 scripts/load.py



ğŸ“‚ Large Files
Due to size limitations, large files are stored externally. You can download them from the link below:

ğŸ”— Download Large Files




